[2022-06-10 17:09:13,829] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: spark_pipeline_dag.extract_data_task manual__2022-06-10T14:09:12.861185+00:00 [queued]>
[2022-06-10 17:09:13,831] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: spark_pipeline_dag.extract_data_task manual__2022-06-10T14:09:12.861185+00:00 [queued]>
[2022-06-10 17:09:13,831] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2022-06-10 17:09:13,831] {taskinstance.py:1357} INFO - Starting attempt 1 of 2
[2022-06-10 17:09:13,831] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2022-06-10 17:09:13,835] {taskinstance.py:1377} INFO - Executing <Task(PythonOperator): extract_data_task> on 2022-06-10 14:09:12.861185+00:00
[2022-06-10 17:09:13,836] {standard_task_runner.py:52} INFO - Started process 1833 to run task
[2022-06-10 17:09:13,838] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'spark_pipeline_dag', 'extract_data_task', 'manual__2022-06-10T14:09:12.861185+00:00', '--job-id', '24', '--raw', '--subdir', 'DAGS_FOLDER/spark_pipeline_dag.py', '--cfg-path', '/var/folders/7p/cr8xz9ln34l55g9c5fr8glxm0000gn/T/tmpjs6zwmfw', '--error-file', '/var/folders/7p/cr8xz9ln34l55g9c5fr8glxm0000gn/T/tmpa1mwjezv']
[2022-06-10 17:09:13,839] {standard_task_runner.py:80} INFO - Job 24: Subtask extract_data_task
[2022-06-10 17:09:13,852] {task_command.py:370} INFO - Running <TaskInstance: spark_pipeline_dag.extract_data_task manual__2022-06-10T14:09:12.861185+00:00 [running]> on host 1.0.0.127.in-addr.arpa
[2022-06-10 17:09:13,869] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=spark_pipeline_dag
AIRFLOW_CTX_TASK_ID=extract_data_task
AIRFLOW_CTX_EXECUTION_DATE=2022-06-10T14:09:12.861185+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2022-06-10T14:09:12.861185+00:00
[2022-06-10 17:09:23,076] {python.py:173} INFO - Done. Returned value was: DataFrame[datetime: string, milliseconds: string, site_id: string, sitezone_id: string, user_id: string, google_id_first: string, yandex_id_first: string]
[2022-06-10 17:09:23,108] {xcom.py:584} ERROR - Could not serialize the XCom value into JSON. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config.
[2022-06-10 17:09:23,108] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 2385, in xcom_push
    XCom.set(
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/models/xcom.py", line 191, in set
    value = cls.serialize_value(
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/models/xcom.py", line 582, in serialize_value
    return json.dumps(value).encode('UTF-8')
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type DataFrame is not JSON serializable
[2022-06-10 17:09:23,110] {taskinstance.py:1395} INFO - Marking task as UP_FOR_RETRY. dag_id=spark_pipeline_dag, task_id=extract_data_task, execution_date=20220610T140912, start_date=20220610T140913, end_date=20220610T140923
[2022-06-10 17:09:23,113] {standard_task_runner.py:92} ERROR - Failed to execute job 24 for task extract_data_task (Object of type DataFrame is not JSON serializable; 1833)
[2022-06-10 17:09:23,125] {local_task_job.py:156} INFO - Task exited with return code 1
[2022-06-10 17:09:23,133] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
