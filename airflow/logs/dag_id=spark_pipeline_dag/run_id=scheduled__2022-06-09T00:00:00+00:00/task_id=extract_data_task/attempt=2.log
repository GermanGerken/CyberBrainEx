[2022-06-10 16:58:24,574] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: spark_pipeline_dag.extract_data_task scheduled__2022-06-09T00:00:00+00:00 [queued]>
[2022-06-10 16:58:24,578] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: spark_pipeline_dag.extract_data_task scheduled__2022-06-09T00:00:00+00:00 [queued]>
[2022-06-10 16:58:24,578] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2022-06-10 16:58:24,578] {taskinstance.py:1357} INFO - Starting attempt 2 of 2
[2022-06-10 16:58:24,578] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2022-06-10 16:58:24,583] {taskinstance.py:1377} INFO - Executing <Task(PythonOperator): extract_data_task> on 2022-06-09 00:00:00+00:00
[2022-06-10 16:58:24,585] {standard_task_runner.py:52} INFO - Started process 13981 to run task
[2022-06-10 16:58:24,587] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'spark_pipeline_dag', 'extract_data_task', 'scheduled__2022-06-09T00:00:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/spark_pipeline_dag.py', '--cfg-path', '/var/folders/7p/cr8xz9ln34l55g9c5fr8glxm0000gn/T/tmp9o9pzhkl', '--error-file', '/var/folders/7p/cr8xz9ln34l55g9c5fr8glxm0000gn/T/tmpxbv3z5qt']
[2022-06-10 16:58:24,588] {standard_task_runner.py:80} INFO - Job 12: Subtask extract_data_task
[2022-06-10 16:58:24,605] {task_command.py:370} INFO - Running <TaskInstance: spark_pipeline_dag.extract_data_task scheduled__2022-06-09T00:00:00+00:00 [running]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2022-06-10 16:58:24,621] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=spark_pipeline_dag
AIRFLOW_CTX_TASK_ID=extract_data_task
AIRFLOW_CTX_EXECUTION_DATE=2022-06-09T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-06-09T00:00:00+00:00
[2022-06-10 16:58:34,120] {xcom.py:584} ERROR - Could not serialize the XCom value into JSON. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config.
[2022-06-10 16:58:34,120] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/operators/python.py", line 171, in execute
    return_value = self.execute_callable()
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/operators/python.py", line 189, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/Users/germangerken/airflow/dags/spark_pipeline_dag.py", line 33, in extract_data
    ti.xcom_push("df", df)
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 71, in wrapper
    return func(*args, session=session, **kwargs)
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 2385, in xcom_push
    XCom.set(
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/models/xcom.py", line 191, in set
    value = cls.serialize_value(
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/models/xcom.py", line 582, in serialize_value
    return json.dumps(value).encode('UTF-8')
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type DataFrame is not JSON serializable
[2022-06-10 16:58:34,124] {taskinstance.py:1395} INFO - Marking task as FAILED. dag_id=spark_pipeline_dag, task_id=extract_data_task, execution_date=20220609T000000, start_date=20220610T135824, end_date=20220610T135834
[2022-06-10 16:58:34,127] {standard_task_runner.py:92} ERROR - Failed to execute job 12 for task extract_data_task (Object of type DataFrame is not JSON serializable; 13981)
