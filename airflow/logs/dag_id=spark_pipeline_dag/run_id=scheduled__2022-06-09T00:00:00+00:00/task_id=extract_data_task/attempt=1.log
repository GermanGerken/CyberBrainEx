[2022-06-10 16:47:26,757] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: spark_pipeline_dag.extract_data_task scheduled__2022-06-09T00:00:00+00:00 [queued]>
[2022-06-10 16:47:26,760] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: spark_pipeline_dag.extract_data_task scheduled__2022-06-09T00:00:00+00:00 [queued]>
[2022-06-10 16:47:26,760] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2022-06-10 16:47:26,760] {taskinstance.py:1357} INFO - Starting attempt 1 of 2
[2022-06-10 16:47:26,760] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2022-06-10 16:47:26,764] {taskinstance.py:1377} INFO - Executing <Task(PythonOperator): extract_data_task> on 2022-06-09 00:00:00+00:00
[2022-06-10 16:47:26,765] {standard_task_runner.py:52} INFO - Started process 12375 to run task
[2022-06-10 16:47:26,767] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'spark_pipeline_dag', 'extract_data_task', 'scheduled__2022-06-09T00:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/spark_pipeline_dag.py', '--cfg-path', '/var/folders/7p/cr8xz9ln34l55g9c5fr8glxm0000gn/T/tmprlnhvful', '--error-file', '/var/folders/7p/cr8xz9ln34l55g9c5fr8glxm0000gn/T/tmpifadbvkd']
[2022-06-10 16:47:26,767] {standard_task_runner.py:80} INFO - Job 7: Subtask extract_data_task
[2022-06-10 16:47:26,781] {task_command.py:370} INFO - Running <TaskInstance: spark_pipeline_dag.extract_data_task scheduled__2022-06-09T00:00:00+00:00 [running]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2022-06-10 16:47:26,795] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=spark_pipeline_dag
AIRFLOW_CTX_TASK_ID=extract_data_task
AIRFLOW_CTX_EXECUTION_DATE=2022-06-09T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-06-09T00:00:00+00:00
[2022-06-10 16:47:36,570] {xcom.py:584} ERROR - Could not serialize the XCom value into JSON. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config.
[2022-06-10 16:47:36,570] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/operators/python.py", line 171, in execute
    return_value = self.execute_callable()
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/operators/python.py", line 189, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/Users/germangerken/airflow/dags/spark_pipeline_dag.py", line 33, in extract_data
    ti.xcom_push("df", df)
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 71, in wrapper
    return func(*args, session=session, **kwargs)
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 2385, in xcom_push
    XCom.set(
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/models/xcom.py", line 191, in set
    value = cls.serialize_value(
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/models/xcom.py", line 582, in serialize_value
    return json.dumps(value).encode('UTF-8')
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type DataFrame is not JSON serializable
[2022-06-10 16:47:36,573] {taskinstance.py:1395} INFO - Marking task as UP_FOR_RETRY. dag_id=spark_pipeline_dag, task_id=extract_data_task, execution_date=20220609T000000, start_date=20220610T134726, end_date=20220610T134736
[2022-06-10 16:47:36,577] {standard_task_runner.py:92} ERROR - Failed to execute job 7 for task extract_data_task (Object of type DataFrame is not JSON serializable; 12375)
[2022-06-10 16:47:36,591] {local_task_job.py:156} INFO - Task exited with return code 1
[2022-06-10 16:47:36,599] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-06-10 16:53:14,110] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: spark_pipeline_dag.extract_data_task scheduled__2022-06-09T00:00:00+00:00 [queued]>
[2022-06-10 16:53:14,113] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: spark_pipeline_dag.extract_data_task scheduled__2022-06-09T00:00:00+00:00 [queued]>
[2022-06-10 16:53:14,114] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2022-06-10 16:53:14,114] {taskinstance.py:1357} INFO - Starting attempt 1 of 2
[2022-06-10 16:53:14,114] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2022-06-10 16:53:14,117] {taskinstance.py:1377} INFO - Executing <Task(PythonOperator): extract_data_task> on 2022-06-09 00:00:00+00:00
[2022-06-10 16:53:14,119] {standard_task_runner.py:52} INFO - Started process 13247 to run task
[2022-06-10 16:53:14,121] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'spark_pipeline_dag', 'extract_data_task', 'scheduled__2022-06-09T00:00:00+00:00', '--job-id', '11', '--raw', '--subdir', 'DAGS_FOLDER/spark_pipeline_dag.py', '--cfg-path', '/var/folders/7p/cr8xz9ln34l55g9c5fr8glxm0000gn/T/tmprbz2e9ou', '--error-file', '/var/folders/7p/cr8xz9ln34l55g9c5fr8glxm0000gn/T/tmpyhw3_80_']
[2022-06-10 16:53:14,122] {standard_task_runner.py:80} INFO - Job 11: Subtask extract_data_task
[2022-06-10 16:53:14,136] {task_command.py:370} INFO - Running <TaskInstance: spark_pipeline_dag.extract_data_task scheduled__2022-06-09T00:00:00+00:00 [running]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2022-06-10 16:53:14,150] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=spark_pipeline_dag
AIRFLOW_CTX_TASK_ID=extract_data_task
AIRFLOW_CTX_EXECUTION_DATE=2022-06-09T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-06-09T00:00:00+00:00
[2022-06-10 16:53:23,411] {xcom.py:584} ERROR - Could not serialize the XCom value into JSON. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config.
[2022-06-10 16:53:23,412] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/operators/python.py", line 171, in execute
    return_value = self.execute_callable()
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/operators/python.py", line 189, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/Users/germangerken/airflow/dags/spark_pipeline_dag.py", line 33, in extract_data
    ti.xcom_push("df", df)
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 71, in wrapper
    return func(*args, session=session, **kwargs)
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 2385, in xcom_push
    XCom.set(
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/utils/session.py", line 68, in wrapper
    return func(*args, **kwargs)
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/models/xcom.py", line 191, in set
    value = cls.serialize_value(
  File "/Users/germangerken/PycharmProjects/CyberBrainExs/venv/lib/python3.10/site-packages/airflow/models/xcom.py", line 582, in serialize_value
    return json.dumps(value).encode('UTF-8')
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type DataFrame is not JSON serializable
[2022-06-10 16:53:23,414] {taskinstance.py:1395} INFO - Marking task as UP_FOR_RETRY. dag_id=spark_pipeline_dag, task_id=extract_data_task, execution_date=20220609T000000, start_date=20220610T135314, end_date=20220610T135323
[2022-06-10 16:53:23,417] {standard_task_runner.py:92} ERROR - Failed to execute job 11 for task extract_data_task (Object of type DataFrame is not JSON serializable; 13247)
[2022-06-10 16:53:23,434] {local_task_job.py:156} INFO - Task exited with return code 1
[2022-06-10 16:53:23,442] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
